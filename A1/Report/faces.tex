%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses j Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{subcaption}
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{amsmath}
\usepackage{float}



% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassTime): \hmwkTitle} % Top center head
%\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for j list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates j new command to include j perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when j page split occurs within j problem environment
\newcommand{\enterProblemHeader}[1]{
%\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when j page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
%\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates j counter to keep track of the number of problems
\setcounter{homeworkProblemCounter}{0}

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes j new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make j section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make j subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Project\ \#1} % Assignment title
\newcommand{\hmwkDueDate}{Monday,\ January\ 29,\ 2018} % Due date
\newcommand{\hmwkClass}{CSC411} % Course/class
\newcommand{\hmwkClassTime}{L0501} % Class/lecture time
\newcommand{\hmwkAuthorName}{Xiangyu Kong} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle
\clearpage
%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put j \clearpage after each problem

\begin{homeworkProblem}

\noindent \textit{Dataset description}

The dataset consists of $1691$ gray-scale $32 \times 32$-pixel of images of $12$ different actor/actress's faces. The angle, lighting and expressions for the faces are all different (as shown in Figure~\ref{fig:uncropped1} and Figure~\ref{fig:uncropped2}). Some faces aren't fully shown (as shown in Figure~\ref{fig:uncropped3}). After cropping their faces out, examining the dataset, most of the faces are correctly cropped given the coordinates (Figure~\ref{fig:cropped1}), but there are still incorrect coordinates that leads to incorrect faces (Figure~\ref{fig:cropped2}). Also, some cropped faces are not aligned together. e.g. Some are center aligned and some are not (Figure~\ref{fig:cropped1} and Figure~\ref{fig:cropped3}).

\begin{figure*}[!ht]
	\begin{subfigure}{.35\textwidth}
		\includegraphics[width=.35\linewidth]{images/1/1.jpg}
		\caption{front}
		\label{fig:uncropped1}
	\end{subfigure}
	\begin{subfigure}{.35\textwidth}
		\includegraphics[width=.35\linewidth]{images/1/2.jpg}
		\caption{side}
		\label{fig:uncropped2}
	\end{subfigure}%
	\begin{subfigure}{.35\textwidth}
		\includegraphics[width=.35\linewidth]{images/1/3.jpg}
		\caption{blocked face}
		\label{fig:uncropped3}
	\end{subfigure}
	\begin{subfigure}{.35\textwidth}
		\includegraphics[width=.35\linewidth]{images/1/4.jpg}
		\caption{correctly cropped}
		\label{fig:cropped1}
	\end{subfigure}%
	\begin{subfigure}{.35\textwidth}
		\includegraphics[width=.35\linewidth]{images/1/5.jpg}
		\caption{incorrect coordinates for face}
		\label{fig:cropped2}
	\end{subfigure}
	\begin{subfigure}{.35\textwidth}
		\includegraphics[width=.35\linewidth]{images/1/6.jpg}
		\caption{face not aligned to the center}
		\label{fig:cropped3}
	\end{subfigure}
	\caption{}
	\label{fig:pcs}
\end{figure*}

\end{homeworkProblem}
\clearpage

%%----------------------------------------------------------------------------------------
%%	PROBLEM 2
%%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
\noindent \textit{Separating the sets.}

The algorithm for separating the sets of data is to first shuffle the data as j list, then through list slicing, first pick $10$ images as the test set, then pick $10$ images as the validation set, and finally the rest are used as the training set.


\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%	PROBLEM 3
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
\noindent \textit{Linear classifier to distinguish between Alec Baldwin from Steve Carell}

Using the divided data sets from part 2, the program trains the linear classifier by gradient descent and then apply the classifier to the validation set and the test set.\\

The functions to compute the output are as follows (all print statements are removed for simplicity):\\
\begin{lstlisting}[language=Python, caption=Gradient Descent]
def grad_descent(loss, dlossdx, x, y, init_theta, alpha):
	eps = 1e-5
	prev_theta = init_theta - 10 * eps
	theta = init_theta.copy()
	max_iter = 100000
	i = 0	
	while norm(theta - prev_theta) > eps and i < max_iter:
		prev_theta = theta.copy()
		theta -= alpha * dlossdx(x, y, theta)
		i += 1
	return theta
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Predict]
def predict(im, theta):
	data = imread("./cropped/" + im) / 225.
	data = reshape(data, 1024)
	data = np.insert(data, 0, 1)
	prediction = np.dot(data, theta)
	return prediction
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Test]
theta = grad_descent(loss, dlossdx, x, y, theta, 0.005)
for im in validation_set:
	prediction = predict(im, theta)
	if im in actor1_validation_set and norm(prediction) > 0.5:
		correct_count += 1
	elif im in actor2_validation_set and norm(prediction) <= 0.5:
		correct_count += 1
\end{lstlisting}

In order for the system to work, I had to keep track of each vector/matrix's shape and when to use their transpose in order for the multiplication to work. Also, I had to tune the alpha value and the iteration value. This is because if the alpha value is too small, the gradient descent will be too slow so more iterations are required, and if the alpha value is too large, numpy will give $nan$ on the trained gradients.

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%	PROBLEM 4
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
\noindent \textit{Plotting thetas}

The produced thetas give figures  Figure~\ref{fig:full_dataset} for using a full dataset to train the theta and Figure~\ref{fig:2_images} for using only two images to train the theta:

\begin{figure}[H]
	\centering
	\includegraphics[width=30mm]{images/4/a_full_theta.jpg}
	\caption{Full dataset}
	\label{fig:full_dataset}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=30mm]{images/4/a_two_theta.jpg}
	\caption{two data}
	\label{fig:2_images}
\end{figure}


\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%	PROBLEM 5
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
	\noindent \textit{Overfitting}
	
	\begin{figure}[!ht]
		\includegraphics[width=\linewidth]{images/5/1.jpg}
		\caption{Overfitting}
		\label{fig:overfitting}
	\end{figure}

	The graph above shows that after the training performance reaches the peak, which is around 88\%, the performance for larger training set decreases and this is because at the peak, the set of data best describes the difference between j male's face and j female's face. After this size, the data starts focusing more on each individual's facial features instead of the features for male and female's faces.
	
	
\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%	PROBLEM 6
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
	\noindent \textit{Multiclass linear classification}
	\begin{enumerate}
		\item 
		\begin{align*}
			\theta^ \top x^{(i)} &=
			\begin{bmatrix}
			\theta_{11}       & \theta_{12} & \theta_{13} & \dots & \theta_{n1} \\
			\theta_{21}       & \theta_{22} & \theta_{23} & \dots & \theta_{n2} \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			\theta_{1k}       & \theta_{2k} & \theta_{3k} & \dots & \theta_{nk}
			\end{bmatrix}
			\begin{bmatrix}
			x^{(i)}_{1} \\
			x^{(i)}_{2}\\
			\vdots \\
			x^{(i)}_{n}
			\end{bmatrix}\\
			&=
			\begin{bmatrix}
			\sum \limits^{n}_{j = 1} \theta_{j1} x^{(i)}_{j}\\
			\sum \limits^{n}_{j = 1} \theta_{j2} x^{(i)}_{j}\\
			\vdots \\
			\sum \limits^{n}_{j = 1} \theta_{jk} x^{(i)}_{j}
			\end{bmatrix}
		\end{align*}
		Then:
		\begin{align*}
		(\theta^ \top x^{(i)} - y^{(i)})^2 &=
		(\begin{bmatrix}
		\sum \limits^{n}_{j = 1} \theta_{j1} x^{(i)}_{j}\\
		\sum \limits^{n}_{j = 1} \theta_{j2} x^{(i)}_{j}\\
		\vdots \\
		\sum \limits^{n}_{j = 1} \theta_{jk} x^{(i)}_{j}
		\end{bmatrix}
		- 
		\begin{bmatrix}
		y^{(i)}_{1} \\
		y^{(i)}_{2}\\
		\vdots \\
		y^{(i)}_{k}
		\end{bmatrix})^2\\
		&= 
		\begin{bmatrix}
		(\sum \limits^{n}_{j = 1} \theta_{j1} x^{(i)}_{j} - y^{(i)}_{1})^2\\
		(\sum \limits^{n}_{j = 1} \theta_{j2} x^{(i)}_{j} - y^{(i)}_{2})^2\\
		\vdots \\
		(\sum \limits^{n}_{j = 1} \theta_{jk} x^{(i)}_{j} - y^{(i)}_{k})^2
		\end{bmatrix}
		\end{align*}
		
		Then
		\begin{align*}
			\dfrac{\partial J}{\partial \theta_{pq}} = \sum \limits_i 2 (\sum \limits^{n}_{j = 1} \theta_{aq} x^{(i)}_a - y^{(i)}_q) x^{(i)}_p
		\end{align*}
		
		
		\item 
		
		Let $m$ be the number of training examples.
		
		$X \text{ is j } n \times m \text{ matrix } \begin{bmatrix}
			x^{(1)}_1 & \dots & x^{(m)}_1\\
			x^{(2)}_2 & \dots & x^{(m)}_2\\
			\vdots & \ddots & \vdots\\
			x^{(1)}_n & \dots & x^{(m)}_n\\
		\end{bmatrix}$
		
		$Y \text{ is j } k \times m \text{ matrix } \begin{bmatrix}
		y^{(1)}_1 & \dots & y^{(m)}_1\\
		y^{(2)}_2 & \dots & y^{(m)}_2\\
		\vdots & \ddots & \vdots\\
		y^{(1)}_n & \dots & y^{(m)}_n\\
		\end{bmatrix}$\\
		
		Then $\nabla J(\theta) = 2 X (\theta^\top X - Y)^\top$.
		
		Then for each row $q$ for $(\theta^\top X - Y) = \begin{bmatrix}
		\sum \limits^{n}_{j = 1} \theta_{aq} x^{(1)}_a - y^{(1)}_q & \sum \limits^{n}_{j = 1} \theta_{aq} x^{(2)}_a - y^{(2)}_q & \dots & \sum \limits^{n}_{j = 1} \theta_{aq} x^{(i)}_a - y^{(m)}_q
		\end{bmatrix}$.
		
		Each row $p$ for $X = \begin{bmatrix}
		X^{(1)}_p & X^{(2)}_p & \dots & X^{(m)}_p
		\end{bmatrix}$
		
		Then the $pq$th entry for $2X(\theta^\top X - Y)^\top$ is $2 \sum \limits^{m}_{i = 1} (\sum \limits^{n}_{j = 1} \theta_{jq} X^{(i)}_j - Y^{(i)}_q) X^{(i)}_p = \dfrac{\partial J}{\partial \theta_{pq}}$
		
		\item 
		Code for cost function and its gradient function in 6.1:
		\begin{lstlisting}[language=Python, caption=Test]
		def loss_m(x, y, theta):
			return sum((np.dot(x, theta.T) - y) ** 2)
		
		def dlossdx_m(x, y, theta):
			return 2*(np.dot(x.T, np.dot(x, theta.T) - y)).T
		\end{lstlisting}
		

	\end{enumerate}
	
	
\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%	PROBLEM 7
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
	\noindent \textit{Running multiclass linear classification}
	The performance on Training set is $ 570 / 621 =  91.7\%$ and on Validation set is $ 50 / 60 =  83.3\%$\\
	$\alpha$ was chosen to be $0.0000001$, which is much smaller than when we trained binary linear classification. This is because when larger alpha is used, the trained theta will contain $nan$.\\
	The output is obtained by choosing the argmax of the prediction array. This is because the higher the predicted value is, the more confident the algorithm is. Then we compare the index of the chosen label with the correct label, and finally we can see if the prediction is correct or not.
	
	
\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%	PROBLEM 8
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
	\noindent \textit{Plotting thetas}
	
	The trained thetas are as follows:
	
	\begin{figure*}[!ht]
		\begin{subfigure}{.35\textwidth}
			\includegraphics[width=.35\linewidth]{images/8/baldwin.jpg}
			\caption{baldwin}
		\end{subfigure}
		\begin{subfigure}{.35\textwidth}
			\includegraphics[width=.35\linewidth]{images/8/bracco.jpg}
			\caption{bracco}
		\end{subfigure}%
		\begin{subfigure}{.35\textwidth}
			\includegraphics[width=.35\linewidth]{images/8/carell.jpg}
			\caption{carell}
		\end{subfigure}
		\begin{subfigure}{.35\textwidth}
			\includegraphics[width=.35\linewidth]{images/8/gilpin.jpg}
			\caption{gilpin}
		\end{subfigure}%
		\begin{subfigure}{.35\textwidth}
			\includegraphics[width=.35\linewidth]{images/8/hader.jpg}
			\caption{hader}
		\end{subfigure}
		\begin{subfigure}{.35\textwidth}
			\includegraphics[width=.35\linewidth]{images/8/harmon.jpg}
			\caption{harmon}
		\end{subfigure}
		\caption{}
		\label{fig:pcs}
	\end{figure*}
	
\end{homeworkProblem}
\clearpage
\end{document}